"下一代架构"点评

周sir 发了一个关于云原生架构设计的内部文档，读完了感触是，这个架构设计得是真心牛B呀！之前还问周sir 这阵子在搞啥，原来是在憋大招。算是我们下一代架构的原型，读完就跟读到一篇非常棒的论文一样的如沐春风，斗胆写一写点评，都是个人理解。

## 解决存储成本问题

首先是副本数量。我们需要 3 副本是因为，节点可能会挂，数据可能会临时不可用，持久存储也不是真正的稳定可靠的，盘可能会坏掉。然而，3 副本也同样意味，存储成本放大了多倍。
产品上云之后，基础设施变得不太一样了，云上面会有弹性的计算节点，有“可靠”的存储节点。假设我们已有了可靠存储的情况下，比如说 aws 的 s3，能否不再存储 3 份副本呢？ 这是架构推衍的第一步。


然后思考一下数据存储的形式是什么。SQL 数据库的行/列/索引，这些实际上都编码成了 kv，所以存储引擎到下层都是 kv 存储。以 rocksdb 为例，**最终持久化的是什么？是 SST 文件**。所以我们希望 1 副本，就是把 SST 文件都存到 s3 里面，只存一份，这一部分 SST 就成了 single source of truth。

顺着下一步推衍，假设只有一份 SST 并且存到 s3 里面了，那存储引擎每次都从 s3 去读数据么？ 那不是相当于本地的存储引擎，挂载一个远程的盘。为了提升性能，我们似乎需要一个缓存层，SST 文件在 s3 存一份，在本地缓存一份。 **本地的 SST 是一份缓存**，这是下一个很重要的设定。


为什么 sst 是可以缓存的

方便缓存的条件是，只读不修改，而 sst 正是这样的。LSM 是把可变的部分放在 memtable，然后周期性的做 compaction 把 memtable 往下层刷，生成 sst。sst 一旦生成之后，在下次做 compaction 之前就不会被修改了。

写放大变小

表面上看，sst 存在 s3 之后会引入额外的网络同步的开销，但实际上写入大是变小的。因为以前 3 副本，各自做 compaction。而将本地数据变缓存，直接数据只存一份到 s3 之后，做 compaction 可以只做一次，其它节点去同步 compaction 之后的结果。

计算量减轻

做 compaction 不仅是写放大的影响，还会消耗很大的计算量。compaction 可以占到 CPU 的 30%，然后性能抖动也经常发生于 compaction：CPU 打满之后请求排队，磁盘写带宽耗尽之后 stall。

另外，compaction 可以挪出去在远程做，因为数据放在 s3 了。或者说，前台任务跟后台任务分离了，如果发挥云上弹性计算节点优势，这种情况可以起一些计算节点去做后台 compaction，存储引擎节点不会引入性能抖动。

## 解决 LSM 的扩展能力




## 对比 Aurora 架构

本地 SST 变成缓存后，有什么好处？
compaction 只需要一遍








LSM 中一些比较核心的概念：WAL + MemTable + SST，记 log 是为了防 crash，然后



----------


适配云的环境，解决现有的问题

存储引擎设计

1. 解决存储成本问题

三副本变一副本

本地 SSD 变缓存

2. 解决引擎的扩展能力

region 物理独立

3. 解决 compaction 

解决加副本调度到平衡的影响

把 aurora 架构里面最有价值的地方“借鉴”过来了

4. 丢数据风险更低

5. 解决 region 数量问题


统计信息搜集


友商的
[How we built a forever-free serverless SQL database](https://www.cockroachlabs.com/blog/how-we-built-cockroachdb-serverless/)

下一篇我将写一下我自己对于 Cloud Native 架构的思考


对 cloud native database 概念的理解：在架构设计上，充分考虑云技术特点的分布式系统



扩展性 -> 弹性

以前需要预估资源的使用量


隔离 -> 支持多租户

pay for what you use ->  对资源使用的精确控制能力
价格是基于资源使用的

instead of pay for server, pay for request!

容错能力

成本


备份恢复


我们在做什么？做适配...

从代码到产品，适配是需要的，但是不够
做云上的自动运维能力
缺乏想象力


AWS Lambda or Google Cloud Functions.

我怎么处理升级？

无限扩容？

多套 tidb-server 可以运行在同一套 tikv-server 上面，相互不影响


## 对比 CockroachDB 多租户架构

SQL 层每个租户是自己的 SQL 层
存储层共享同一个，在 key 的编码里面添加了 tenant id 来划分，做好访问控制

对 kv 层，要做好 tenant 级别的读写请求的 measure

进程的 CPU 和内存的使用量... 通过 cgroup 搞定

什么时候应该扩缩容？

服务质量保证

元数据管理
